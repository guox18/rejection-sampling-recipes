# Rejection Sampling Recipes

**Reproducible recipes for rejection sampling in synthetic data generation.**

## Why RSR?

The community already has great tools for inference (vLLM, SGLang), training (LLaMA-Factory, veRL), and evaluation (lm-eval-harness). But when it comes to **rejection sampling / Best-of-N** for distillation or RL data curation, most of us end up writing one-off scripts.

RSR handles the trivial-but-important details:

- **Truncation detection** â€” Detects and discards truncated responses
- **Smart early stopping** â€” Stop sampling once you have what you need
- **Checkpoint & resume** â€” Shard-based storage for large-scale sampling
- **Quality stats** â€” Pass rates, token distributions, saved automatically

## Core Concepts

### Pipeline Flow

```mermaid
flowchart LR
    subgraph Input
        A[("ğŸ“„ Data")]
    end

    subgraph Rollout["Rollout Loop (per shard)"]
        direction TB
        B["ğŸ² Sampler"]
        C["âœ“ Verifier"]
        B --> C
    end

    subgraph Output
        E["ğŸ“Š Formatter"]
        F[("ğŸ¯ Train Data")]
        E --> F
    end

    A --> Rollout
    Rollout --> E

    style A fill:#e3f2fd,stroke:#1565c0
    style B fill:#fff3e0,stroke:#e65100
    style C fill:#f3e5f5,stroke:#7b1fa2
    style E fill:#e8f5e9,stroke:#2e7d32
    style F fill:#e8f5e9,stroke:#2e7d32
```

### Components

| Component | Description | Options |
|-----------|-------------|---------|
| **Sampler** | Generates responses from LLM | `vllm-offline`, `openai-compatible-api` |
| **Verifier** | Scores responses against ground truth | `mcq-rlvr`, `mcq-llm-as-judge`, `math-rlvr` |
| **Formatter** | Converts scored rollouts to training data | `sft`, `dpo`, `multi_sft` |

### Data Flow

```
Input (prompts + metadata)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sampling Loop (per prompt) â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Step 1: sample N       â”‚ â”‚
â”‚  â”‚ Step 2: verify scores  â”‚ â”‚
â”‚  â”‚ Step 3: check early    â”‚â—€â”¤â”€â”€ repeat until satisfied
â”‚  â”‚         stop condition â”‚ â”‚     or max_steps reached
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Rollouts (responses + scores)
    â†“
Formatter â†’ Training Data (SFT/DPO/Multi-SFT)
```

## Output Structure

```
output/20251206_143052/
â”œâ”€â”€ config.yaml          # Experiment config (auto-saved)
â”œâ”€â”€ state.json           # Progress & checkpoint state
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input.jsonl      # Preprocessed input data
â”œâ”€â”€ rollout/             # Rollout results (sharded)
â”‚   â”œâ”€â”€ shard_0000.jsonl
â”‚   â””â”€â”€ ...
â”œâ”€â”€ train/               # Training-ready data
â”‚   â”œâ”€â”€ sft.jsonl
â”‚   â””â”€â”€ dpo.jsonl
â””â”€â”€ summary/
    â””â”€â”€ stats.json       # Quality statistics
```

## Next Steps

- [Getting Started](getting-started.md) â€” Run your first sampling job
- [Configuration](configuration.md) â€” Full parameter reference
- [Recipes](recipes.md) â€” SFT, DPO, Multi-SFT workflows
- [Early Stop](early-stop.md) â€” Smart sampling termination
- [Reliability](reliability.md) â€” Truncation handling & checkpoints
