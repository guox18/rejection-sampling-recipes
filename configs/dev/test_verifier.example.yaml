# Configuration for MCQ Verifier Testing Tool
# Copy this file to test_verifier.yaml and fill in your API keys
#
# Usage:
#   cp configs/dev/test_verifier.example.yaml configs/dev/test_verifier.yaml
#   # Edit test_verifier.yaml with your actual API keys
#   python scripts/dev_test_mcq_verifier.py -i data/xxx.jsonl

# Default sampling settings
sampling:
  size: 10
  seed: 42

# Model configurations for generating test responses
# Add your models here
models:
  qwen:
    name: "qwen2.5-32b"
    base_url: "http://your-qwen-endpoint:8004/v1"
    model_id: "qwen"
    api_key: "your-api-key-here"
    temperature: 0.3
    max_tokens: 4096
    verify_ssl: true

  deepseek-r1:
    name: "deepseek-r1"
    base_url: "https://your-deepseek-endpoint/v1/"
    model_id: "deepseek-r1"
    api_key: "your-api-key-here"
    verify_ssl: false
    temperature: 0.6
    max_tokens: 8192

  deepseek-v3:
    name: "deepseek-v3"
    base_url: "https://your-deepseek-endpoint/v1/"
    model_id: "deepseek-v3"
    api_key: "your-api-key-here"
    verify_ssl: false
    temperature: 0.6
    max_tokens: 4096

test_models: ["qwen", "deepseek-r1", "deepseek-v3"]

# LLM Judge configuration (for comparing against rule-based verifier)
llm_judge:
  base_url: "http://your-judge-endpoint:8004/v1"
  model: "qwen"
  api_key: "your-api-key-here"

# Output paths (relative to project root)
paths:
  output_dir: ".dev/mcq_verifier/collected"
  results_file: "verifier_test_results.json"
  cache_file: "llm_judge_cache.json"
