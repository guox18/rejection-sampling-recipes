# Default config - see also: sft.yaml, multi_sft.yaml, dpo.yaml

hydra:
  run:
    dir: output/hydra/${now:%Y-%m-%d}/${now:%H-%M-%S}

data:
  input_path: ???                   # Path to input jsonl file
  preprocess:
    transform: null                 # Data transform script (null = no transform)

work_dir: null                      # Output directory (null = auto timestamp)
verbose: false                      # Enable verbose logging

sampling:
  max_rollouts: 16                  # Target: max valid rollouts to collect per prompt
  step_size: 4                      # Batch size: responses sampled per round
  max_steps: 8                      # Hard limit: max sampling rounds (max_steps × step_size should be ≥ max_rollouts to handle truncation)
  early_stop: true                  # Stop when formatter is satisfied

sampler:
  type: openai-compatible-api       # Sampler type: openai-compatible-api | vllm-offline
  model: ???                        # Model name
  base_url: null                    # API endpoint (null = OpenAI default)
  api_key: null                     # API key (null = use env OPENAI_API_KEY)
  model_path: null                  # Local model path (vllm-offline only)
  tensor_parallel_size: 1           # GPUs per worker (vllm-offline only)
  data_parallel_size: null          # Number of workers (vllm-offline only)
  gpu_memory_utilization: 0.9       # GPU memory ratio (vllm-offline only)
  temperature: 0.7                  # Sampling temperature
  max_tokens: 2048                  # Max tokens per response
  top_p: 1.0                        # Nucleus sampling threshold
  concurrent_requests: 50           # Concurrent API requests
  timeout: 300                      # Request timeout in seconds
  drop_truncated: true              # Drop truncated responses
  extra_params: {}                  # Extra API parameters

verifier:
  type: mcq-rlvr                    # Verifier type: mcq-rlvr | math-rlvr | mcq-llm-as-judge
  model: null                       # Judge model (mcq-llm-as-judge only)
  base_url: null                    # Judge API endpoint (mcq-llm-as-judge only)
  api_key: null                     # Judge API key (mcq-llm-as-judge only)
  temperature: 0.0                  # Judge temperature (mcq-llm-as-judge only)
  max_tokens: 10                    # Judge max tokens (mcq-llm-as-judge only)

formatter:
  - type: sft                       # Formatter type: sft | multi_sft | dpo
    pass_threshold: 1.0             # Score >= this is passed
    fail_threshold: 0.0             # Score <= this is failed
    # num_responses: 32             # Required unique responses (multi_sft only)

shard:
  size: 10000                       # Items per shard file
